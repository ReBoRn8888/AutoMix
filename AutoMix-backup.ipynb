{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import pprint\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import ot\n",
    "\n",
    "def add_path(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "lib_path = os.path.abspath('models')\n",
    "add_path(lib_path)\n",
    "\n",
    "from models import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLoss(nn.Module):\n",
    "    def __init__(self, mtype='baseline'):\n",
    "        super(myLoss, self).__init__()\n",
    "        self.type = mtype.lower()\n",
    "        \n",
    "    def forward(self, pred, truth):\n",
    "#         pred += 1e-9\n",
    "#         truth += 1e-9\n",
    "        if(self.type in ['bc', 'automix']):\n",
    "#             entropy = - torch.sum(truth[truth > 0] * torch.log(truth[truth > 0]), 1)\n",
    "            entropy = - torch.sum((truth+1e-9) * torch.log((truth+1e-9)), 1)\n",
    "            crossEntropy = - torch.sum((truth+1e-9) * F.log_softmax((pred+1e-9), 1), 1)\n",
    "            loss = crossEntropy - entropy\n",
    "        else:\n",
    "            pred = F.log_softmax((pred+1e-9), 1)\n",
    "            loss = -torch.sum((pred+1e-9) * (truth+1e-9), 1)\n",
    "        return loss\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, images, labels, classes=None, transform=None, mtype='baseline', onehot=True):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.type = mtype.lower()\n",
    "        self.onehot = onehot\n",
    "        print(self.type)\n",
    "        \n",
    "    def to_onehot(self, label):\n",
    "        label = torch.unsqueeze(torch.unsqueeze(label, 0), 1)\n",
    "        label = torch.zeros(1, self.num_classes).scatter_(1, label, 1)\n",
    "        label = torch.squeeze(label)\n",
    "        return label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if(self.type == 'bc'):\n",
    "            while True:  # Select two training examples\n",
    "                id1 = index\n",
    "                image1, label1 = self.images[id1], self.labels[id1]\n",
    "                id2 = np.random.randint(0, self.__len__() - 1)\n",
    "                image2, label2 = self.images[id2], self.labels[id2]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            if(self.transform):\n",
    "                image1 = self.transform(Image.fromarray(np.uint8(image1)))\n",
    "                image2 = self.transform(Image.fromarray(np.uint8(image2)))\n",
    "            # Mix two images\n",
    "            r = torch.rand(1)\n",
    "            g1 = torch.std(image1)\n",
    "            g2 = torch.std(image2)\n",
    "            p = 1.0 / (1 + g1 / g2 * (1 - r) / r)\n",
    "            image = ((image1 * p + image2 * (1 - p)) / np.sqrt(p ** 2 + (1 - p) ** 2))\n",
    "            \n",
    "            # Mix two labels\n",
    "            label1 = self.to_onehot(label1)\n",
    "            label2 = self.to_onehot(label2)\n",
    "            label = (label1 * r + label2 * (1 - r)).float()\n",
    "            return image, label\n",
    "        else:\n",
    "            image, label = self.images[index], self.labels[index]\n",
    "            if(self.onehot):\n",
    "                label = self.to_onehot(label)\n",
    "            if(self.transform):\n",
    "                image = self.transform(Image.fromarray(np.uint8(image)))\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 128\n",
    "test_batch_size = 100\n",
    "netList = [\"MyNet\", \"ResNet18\"]\n",
    "netIndex = 1\n",
    "dataName = [\"CIFAR10\", \"CIFAR100\", \"MNIST\", \"FASHION-MNIST\", \"GTSRB\", \"MIML\"]\n",
    "dataIndex = 0\n",
    "methodList = ['baseline', 'bc', 'mixup', 'automix', 'adamixup']\n",
    "methodIndex = 3\n",
    "print(netList[netIndex], dataName[dataIndex], methodList[methodIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_mean(tensor, mean, std):\n",
    "    if not torchvision.transforms.functional._is_tensor_image(tensor):\n",
    "        raise TypeError('tensor is not a torch image.')\n",
    "    # TODO: make efficient\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.sub_(m).sub_(torch.mean(t)).div_(s)\n",
    "    return tensor\n",
    "\n",
    "class ZeroMean(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return zero_mean(tensor, self.mean, self.std)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "if(dataName[dataIndex] == 'CIFAR10'):\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    num_classes = len(classes)\n",
    "    shape = (3, 32, 32)\n",
    "    lrDecayStep = [150, 225]\n",
    "    n_epochs = 300\n",
    "    \n",
    "    if(methodList[methodIndex] == 'bc'):\n",
    "        normalize = ZeroMean\n",
    "    else:\n",
    "        normalize = transforms.Normalize\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "#         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    oriTrainDataset = datasets.CIFAR10(root='Dataset/CIFAR10', \n",
    "                                            train=True, download=True, transform=transform_train)\n",
    "    images = oriTrainDataset.data\n",
    "    labels = torch.Tensor(oriTrainDataset.targets).long()\n",
    "    trainDataset = myDataset(images, labels, classes, transform_train, mtype=methodList[methodIndex])\n",
    "    trainLoader = DataLoader(trainDataset, batch_size=train_batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    oriTestDataset = datasets.CIFAR10(root='Dataset/CIFAR10', \n",
    "                                           train=False, download=False, transform=transform_test)\n",
    "    testImages = oriTestDataset.data\n",
    "    testLabels = torch.Tensor(oriTestDataset.targets).long()\n",
    "    testDataset = myDataset(testImages, testLabels, classes, transform_test)\n",
    "    testLoader = DataLoader(testDataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n",
    "if(dataName[dataIndex] == 'CIFAR100'):\n",
    "    classes = ['{}'.format(i) for i in range(100)]\n",
    "    num_classes = len(classes)\n",
    "    shape = (3, 32, 32)\n",
    "    lrDecayStep = [150, 225]\n",
    "    n_epochs = 300\n",
    "    \n",
    "    if(methodList[methodIndex] == 'bc'):\n",
    "        normalize = ZeroMean\n",
    "    else:\n",
    "        normalize = transforms.Normalize\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "#         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    oriTrainDataset = datasets.CIFAR100(root='Dataset/CIFAR100', \n",
    "                                            train=True, download=True, transform=transform_train)\n",
    "    images = oriTrainDataset.data\n",
    "    labels = torch.Tensor(oriTrainDataset.targets).long()\n",
    "    trainDataset = myDataset(images, labels, classes, transform_train, mtype=methodList[methodIndex])\n",
    "    trainLoader = DataLoader(trainDataset, batch_size=train_batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    oriTestDataset = datasets.CIFAR100(root='Dataset/CIFAR100', \n",
    "                                           train=False, download=False, transform=transform_test)\n",
    "    testImages = oriTestDataset.data\n",
    "    testLabels = torch.Tensor(oriTestDataset.targets).long()\n",
    "    testDataset = myDataset(testImages, testLabels, classes, transform_test)\n",
    "    testLoader = DataLoader(testDataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n",
    "elif(dataName[dataIndex] == 'GTSRB'):\n",
    "    classes = ['{}'.format(i) for i in range(43)]\n",
    "    num_classes = len(classes)\n",
    "    shape = (3, 28, 28)\n",
    "    lrDecayStep = [50, 75]\n",
    "    n_epochs = 100\n",
    "\n",
    "    if(methodList[methodIndex] == 'bc'):\n",
    "        normalize = ZeroMean\n",
    "    else:\n",
    "        normalize = transforms.Normalize\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.3352, 0.3173, 0.3584), (0.2662, 0.2563, 0.2727))\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.3352, 0.3173, 0.3584), (0.2662, 0.2563, 0.2727))\n",
    "    ])\n",
    "    \n",
    "    with open('Dataset/GTSRB/39209-all/images.pkl', 'rb') as f:\n",
    "        images = torch.from_numpy(pickle.load(f)).float()\n",
    "    with open('Dataset/GTSRB/39209-all/labels.pkl', 'rb') as f:\n",
    "        labels = torch.from_numpy(pickle.load(f))\n",
    "        labels = torch.argmax(labels, 1)\n",
    "    with open('Dataset/GTSRB/39209-all/testImages.pkl', 'rb') as f:\n",
    "        testImages = torch.from_numpy(pickle.load(f)).float()\n",
    "    with open('Dataset/GTSRB/39209-all/testLabels.pkl', 'rb') as f:\n",
    "        testLabels = torch.from_numpy(pickle.load(f))\n",
    "        testLabels = torch.argmax(testLabels, 1)\n",
    "    trainDataset = myDataset(images, labels, classes, transform_train, mtype=methodList[methodIndex])\n",
    "    trainLoader = DataLoader(trainDataset, batch_size=train_batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    testDataset = myDataset(testImages, testLabels, classes, transform_test)\n",
    "    testLoader = DataLoader(testDataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n",
    "elif(dataName[dataIndex] == 'MNIST'):\n",
    "    classes = ['{}'.format(i) for i in range(10)]\n",
    "    num_classes = len(classes)\n",
    "    shape = (1, 28, 28)\n",
    "    lrDecayStep = [50, 75]\n",
    "    n_epochs = 100\n",
    "\n",
    "    if(methodList[methodIndex] == 'bc'):\n",
    "        normalize = ZeroMean\n",
    "    else:\n",
    "        normalize = transforms.Normalize\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    oriTrainDataset = datasets.MNIST('Dataset/MNIST', \n",
    "                                  train=True, download=False, transform=transform_train)\n",
    "    images = oriTrainDataset.data\n",
    "    labels = oriTrainDataset.targets\n",
    "    trainDataset = myDataset(images, labels, classes, transform_train, mtype=methodList[methodIndex])\n",
    "    trainLoader = DataLoader(trainDataset, batch_size=train_batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    oriTestDataset = datasets.MNIST('Dataset/MNIST', \n",
    "                                 train=False, download=False, transform=transform_test)\n",
    "    testImages = oriTestDataset.data\n",
    "    testLabels = oriTestDataset.targets\n",
    "    testDataset = myDataset(testImages, testLabels, classes, transform_test)\n",
    "    testLoader = DataLoader(testDataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n",
    "elif(dataName[dataIndex] == 'FASHION-MNIST'):\n",
    "    classes = ['{}'.format(i) for i in range(10)]\n",
    "    num_classes = len(classes)\n",
    "    shape = (1, 28, 28)\n",
    "    lrDecayStep = [50, 75]\n",
    "    n_epochs = 100\n",
    "    \n",
    "    if(methodList[methodIndex] == 'bc'):\n",
    "        normalize = ZeroMean\n",
    "    else:\n",
    "        normalize = transforms.Normalize\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.2860,), (0.3530,))\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         normalize((0.2860,), (0.3530,))\n",
    "    ])\n",
    "\n",
    "    oriTrainDataset = datasets.FashionMNIST('Dataset/Fashion-MNIST', \n",
    "                                  train=True, download=True, transform=transform_train)\n",
    "    images = oriTrainDataset.data\n",
    "    labels = oriTrainDataset.targets\n",
    "    trainDataset = myDataset(images, labels, classes, transform_train, mtype=methodList[methodIndex])\n",
    "    trainLoader = DataLoader(trainDataset, batch_size=train_batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    oriTestDataset = datasets.FashionMNIST('Dataset/Fashion-MNIST', \n",
    "                                 train=False, download=False, transform=transform_test)\n",
    "    testImages = oriTestDataset.data\n",
    "    testLabels = oriTestDataset.targets\n",
    "    testDataset = myDataset(testImages, testLabels, classes, transform_test)\n",
    "    testLoader = DataLoader(testDataset, batch_size=test_batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    perm = np.random.permutation(len(x))\n",
    "    x_shuffle = x[perm]\n",
    "    y_shuffle = y[perm]\n",
    "    return x_shuffle, y_shuffle\n",
    "\n",
    "def get_cls_result(net, dataLoader):\n",
    "    y_score, y_true = [], []\n",
    "    for i, data in enumerate(dataLoader, 0):\n",
    "        inputs, labels = data\n",
    "        y_true.extend(labels.numpy())\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = net(inputs)\n",
    "            y_score.extend(outputs.cpu().detach().numpy())\n",
    "    y_score = np.array(y_score)\n",
    "    y_true = np.array(y_true)\n",
    "    return y_true, y_score\n",
    "\n",
    "def get_roc_data(y_true, y_score, n_classes, rocType='micro'):\n",
    "    fpr = dict() \n",
    "    tpr = dict() \n",
    "    thres = dict()\n",
    "    roc_auc = dict() \n",
    "    for i in range(n_classes): \n",
    "        fpr[i], tpr[i], thres[i] = roc_curve(y_true[:, i], y_score[:, i]) \n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    if(rocType == \"micro\"):\n",
    "        fprOut, tprOut, _ = roc_curve(y_true.ravel(), y_score.ravel())\n",
    "    elif(rocType == \"macro\"):\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)])) \n",
    "        mean_tpr = np.zeros_like(all_fpr) \n",
    "        for i in range(n_classes): \n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i]) \n",
    "        mean_tpr /= n_classes \n",
    "        fprOut = all_fpr \n",
    "        tprOut = mean_tpr \n",
    "    aucOut = auc(fprOut, tprOut)\n",
    "    ROC = dict()\n",
    "    ROC['fpr'], ROC['tpr'], ROC['auc'] = fprOut, tprOut, aucOut\n",
    "    return ROC\n",
    "\n",
    "def ROC_plot(sorted_ROC):\n",
    "    lw=2\n",
    "    plt.figure(figsize=(10, 10)) \n",
    "    title = \"{} AUC : \".format(dataName[dataIndex])\n",
    "    for i, ROC in enumerate(sorted_ROC):\n",
    "        if(i == len(sorted_ROC) - 1):\n",
    "            title += \"{}\".format(ROC[0])\n",
    "        else:\n",
    "            title += \"{} > \".format(ROC[0])\n",
    "        plt.plot(ROC[1][\"fpr\"], ROC[1][\"tpr\"], \n",
    "                 label='{}\\'s AUC = {}'.format(ROC[0], ROC[1][\"auc\"]), linestyle='--', linewidth=lw) \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw) \n",
    "    plt.xlim([-0.02, 1.02])\n",
    "    plt.ylim([-0.02, 1.02]) \n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.title(title) \n",
    "    plt.legend(loc=\"lower right\") \n",
    "#     plt.savefig(\"{}/{}-{}{}-ROC.jpg\".format(path, dataName[dataIndex], num_examples, suffix))\n",
    "    plt.show()\n",
    "\n",
    "def distribution(labels, type='normal'):\n",
    "    result = {}\n",
    "    if(type == 'onehot'):\n",
    "        ll = np.argmax(labels, axis=1)\n",
    "    else:\n",
    "        ll = labels\n",
    "    for i in set(ll):\n",
    "        result[i] = ll.tolist().count(i)\n",
    "    print(\"Label distribution: {}\".format(result))\n",
    "\n",
    "def cal_mean_std(dataset):\n",
    "    dataLoader = DataLoader(dataset, batch_size=dataset.__len__(), shuffle=True, num_workers=1)\n",
    "    it = iter(dataLoader)\n",
    "    images, labels = it.next()\n",
    "    images = np.array(images)\n",
    "    mean = np.round(np.mean(images, axis=(0, 2, 3)), 4)\n",
    "    std = np.round(np.std(images, axis=(0, 2, 3)), 4)\n",
    "    return mean, std\n",
    "\n",
    "def eval_total(net, dataLoader):\n",
    "    start = time.time()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataLoader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            predicted = torch.argmax(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(torch.argmax(labels, 1)).cpu().sum().float().item()\n",
    "\n",
    "    accTotal = 100 * correct / total\n",
    "    duration = time.time() - start\n",
    "    print('Accuracy of the network on the 10000 test images: {:.2f}% ({:.0f}mins {:.2f}s)'\n",
    "          .format(accTotal, duration // 60, duration % 60))\n",
    "    return accTotal\n",
    "\n",
    "def eval_per_class(net, dataLoader, classes):\n",
    "    num_classes = len(classes)\n",
    "    start = time.time()\n",
    "    class_correct = list(0. for i in range(num_classes))\n",
    "    class_total = list(0. for i in range(num_classes))\n",
    "    with torch.no_grad():\n",
    "        for data in dataLoader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            c = predicted.eq(torch.argmax(labels, 1)).cpu().squeeze()\n",
    "            for i in range(len(c)):\n",
    "                label = torch.argmax(labels[i]).item()\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    print('class_correct\\t:\\t{}'.format(class_correct))\n",
    "    print('class_total\\t:\\t{}'.format(class_total))\n",
    "    accPerClass = dict()\n",
    "    for i in range(num_classes):\n",
    "        accPerClass[classes[i]] = 0 if class_correct[i] == 0 else '{:.2f}%'.format(100 * class_correct[i] / class_total[i])\n",
    "    duration = time.time() - start\n",
    "    print('Per class accuracy :')\n",
    "    pprint.pprint(accPerClass)\n",
    "    print('Duration for accPerClass : {:.0f}mins {:.2f}s'.format(duration // 60, duration % 60))\n",
    "    return accPerClass\n",
    "\n",
    "def plot_acc_loss(log, type, prefix='', suffix=''):\n",
    "    trainAcc = log['acc']['train']\n",
    "    trainLoss = log['loss']['train']\n",
    "    valAcc = log['acc']['val']\n",
    "    valLoss = log['loss']['val']\n",
    "    if(type == 'loss'):\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.plot(trainLoss, label='Train_loss')\n",
    "        plt.plot(valLoss, label='Test_loss')\n",
    "        plt.title('Epoch - Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        figName = '{}loss{}.png'.format(prefix, suffix)\n",
    "    elif(type == 'accuracy'):\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.plot(trainAcc, label='Train_acc')\n",
    "        plt.plot(valAcc, label='Test_acc')\n",
    "        plt.title('Epoch - Acuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "#         plt.ylim(0, 1.01)\n",
    "        plt.legend()\n",
    "        figName = '{}accuracy{}.png'.format(prefix, suffix)\n",
    "    elif(type == 'both'):\n",
    "        fig, ax1 = plt.subplots(figsize=(7, 5))\n",
    "        ax2 = plt.twinx()\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        plt.ylim(0, 1.01)\n",
    "        plt.title('Loss & Accuracy')\n",
    "\n",
    "        l_trainLoss, = ax1.plot(trainLoss)\n",
    "        l_testLoss, = ax1.plot(valLoss)\n",
    "        l_trainAcc, = ax2.plot(trainAcc)\n",
    "        l_testAcc, = ax2.plot(valAcc)\n",
    "        plt.legend([l_trainLoss, l_testLoss, l_trainAcc, l_testAcc],\n",
    "                  ['Train_loss', 'Test_loss', 'Train_acc', 'Test_acc'])\n",
    "        figName = '{}loss_accuracy{}.png'.format(prefix, suffix)\n",
    "\n",
    "    plt.grid(linewidth=1, linestyle='-.')\n",
    "    plt.savefig(os.path.join(modelPath, figName), dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def get_shuffled_data(x, y):\n",
    "    index = torch.randperm(x.shape[0])\n",
    "    x_a, x_b = x, x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return x_a, x_b, y_a, y_b\n",
    "\n",
    "def cal_ssim(img1, img2):\n",
    "    img1 = img1.cpu().detach().numpy()\n",
    "    img2 = img2.cpu().detach().numpy()\n",
    "    k1 = 0.01\n",
    "    k2 = 0.03\n",
    "    l = 255\n",
    "    C1 = (k1 * l) ** 2\n",
    "    C2 = (k2 * l) ** 2\n",
    "    C3 = C2 / 2\n",
    "    mean1 = np.mean(img1, axis=(1, 2, 3))\n",
    "    mean2 = np.mean(img2, axis=(1, 2, 3))\n",
    "    var1 = np.var(img1, axis=(1, 2, 3))\n",
    "    var2 = np.var(img2, axis=(1, 2, 3))\n",
    "    std1 = np.sqrt(var1)\n",
    "    std2 = np.sqrt(var2)\n",
    "    cov = np.mean((img1 - mean1[:, np.newaxis, np.newaxis, np.newaxis]) * \n",
    "                  (img2 - mean2[:, np.newaxis, np.newaxis, np.newaxis]), axis=(1, 2, 3))\n",
    "    L = (2 * mean1 * mean2 + C1) / (mean1 ** 2 + mean2 ** 2 + C1)\n",
    "    C = (2 * std1 * std2 + C2) / (std1 ** 2 + std2 ** 2 + C2)\n",
    "    S = (cov + C3) / (std1 * std2 + C3)\n",
    "    ssim = L * C * S\n",
    "    return torch.from_numpy(ssim).to(device)\n",
    "\n",
    "def Euclidean_distance(P, Q):\n",
    "    P = F.softmax(P.view(P.shape[0], -1), 1) + 1e-9\n",
    "    Q = F.softmax(Q.view(Q.shape[0], -1), 1) + 1e-9\n",
    "    return ((P - Q) ** 2).sum(1).sqrt()\n",
    "\n",
    "def KL_divergence(P, Q):\n",
    "    P = F.softmax(P.view(P.shape[0], -1), 1) + 1e-9\n",
    "    Q = F.softmax(Q.view(Q.shape[0], -1), 1) + 1e-9\n",
    "    return torch.sum(P * P.log() - P * Q.log(), 1)\n",
    "\n",
    "def JS_divergence(P, Q):\n",
    "    return 0.5 * KL_divergence(P, (P + Q) / 2) + 0.5 * KL_divergence(Q, (P + Q) / 2)\n",
    "\n",
    "def EMD(P, Q):\n",
    "    a = np.ones(shape[0] * shape[1] * shape[2]) / (shape[0] * shape[1] * shape[2])\n",
    "    b = np.ones(shape[0] * shape[1] * shape[2]) / (shape[0] * shape[1] * shape[2])\n",
    "    dis = []\n",
    "    for i in range(len(P)):\n",
    "        M = ot.dist(P[i].view(-1, 1).cpu().detach().numpy(), Q[i].view(-1, 1).cpu().detach().numpy(), 'euclidean')\n",
    "        d = ot.emd2(a, b, M)\n",
    "        dis.append(d)\n",
    "    return torch.Tensor(dis)\n",
    "\n",
    "def sinkhorn(P, Q):\n",
    "    a = np.ones(shape[0] * shape[1] * shape[2]) / (shape[0] * shape[1] * shape[2])\n",
    "    b = np.ones(shape[0] * shape[1] * shape[2]) / (shape[0] * shape[1] * shape[2])\n",
    "    dis = []\n",
    "    for i in range(len(P)):\n",
    "        M = ot.dist(P[i].view(-1, 1).cpu().detach().numpy(), Q[i].view(-1, 1).cpu().detach().numpy(), 'euclidean')\n",
    "        d = ot.sinkhorn2(a, b, M, 1, numItermax=100)\n",
    "        dis.append(d)\n",
    "    return torch.Tensor(dis)\n",
    "\n",
    "def norm1(P, Q):\n",
    "#     P = P.view(P.shape[0], -1)\n",
    "#     Q = Q.view(Q.shape[0], -1)\n",
    "    return (P - Q).abs().mean(1).mean(1).mean(1)\n",
    "#     return torch.norm(P - Q, p=1, dim=[1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(optimizer, n_epochs, trainDataset, trainLoader, valDataset, valLoader):\n",
    "    lossLog = dict({'train': [], 'val': []})\n",
    "    accLog = dict({'train': [], 'val': []})\n",
    "    dataSet = {'train': trainDataset, 'val': valDataset}\n",
    "    dataLoader = {'train': trainLoader, 'val': valLoader}\n",
    "    dataSize = {x: dataSet[x].__len__() for x in ['train', 'val']}\n",
    "    batchSize = {'train': train_batch_size, 'val': test_batch_size}\n",
    "    iterNum = {x: np.ceil(dataSize[x] / batchSize[x]).astype('int32') for x in ['train', 'val']}\n",
    "    print('dataSize: {}'.format(dataSize))\n",
    "    print('batchSize: {}'.format(batchSize))\n",
    "    print('iterNum: {}'.format(iterNum))\n",
    "    best_acc = 0.0\n",
    "    start = time.time()\n",
    "    for epoch in tqdm_notebook(range(n_epochs), desc='Epoch'):  # loop over the dataset multiple times\n",
    "        print('Epoch {}/{} lr = {}'.format(epoch+1, n_epochs, optimizer.param_groups[0]['lr']))\n",
    "        print('-' * 10)\n",
    "        epochStart = time.time()\n",
    "        for phase in ['train', 'val']:\n",
    "            if(phase == 'train'):\n",
    "                exp_lr_scheduler.step()\n",
    "                net.train()  # Set model to training mode\n",
    "            else:\n",
    "                net.eval()   # Set model to evaluate mode\n",
    "            running_loss = []\n",
    "            running_corrects = 0\n",
    "            running_cnt = 0\n",
    "            for i, data in enumerate(dataLoader[phase], 0):\n",
    "                sys.stdout.flush()\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                if(methodList[methodIndex] == 'mixup' and phase == 'train'):\n",
    "                    inputs, labels_a, labels_b, lam = mixup_data(inputs, labels)\n",
    "                elif(methodList[methodIndex] in ['automix', 'adamixup'] and phase == 'train'):\n",
    "                    inputs_a, inputs_b, labels_a, labels_b = get_shuffled_data(inputs, labels)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if(methodList[methodIndex] == 'adamixup' and phase == 'train'):\n",
    "                        midIdx = int(len(inputs_a) / 2)\n",
    "                        inputs_unseen = torch.cat([inputs_a[midIdx:]], 0)\n",
    "                        labels_unseen = torch.cat([labels_a[midIdx:]], 0)\n",
    "                        inputs_a, inputs_b = inputs_a[:midIdx], inputs_b[:midIdx]\n",
    "                        labels_a, labels_b = labels_a[:midIdx], labels_b[:midIdx]\n",
    "                        m1, m2 = torch.randn(inputs_a.shape).to(device), torch.randn(inputs_b.shape).to(device)\n",
    "                        inputs = (m1 * inputs_a + m2 * inputs_b) / 2\n",
    "\n",
    "                        gate = PRG_net(inputs)\n",
    "                        gate = torch.softmax(gate, 1)\n",
    "                        alpha, alpha2, _ = torch.split(gate, [1, 1, 1], 1)\n",
    "                        uni = torch.rand([len(inputs_a), 1]).to(device)\n",
    "                        weight = alpha + uni * alpha2\n",
    "                        \n",
    "                        x_weight = weight.view(len(inputs_a), 1, 1, 1)\n",
    "                        y_weight = weight.view(len(inputs_a), 1)\n",
    "                        x_mix = inputs_a * x_weight + inputs_b * (1 - x_weight)\n",
    "                        y_mix = labels_a * y_weight + labels_b * (1 - y_weight)\n",
    "                        x_all = torch.cat([x_mix, inputs_a], 0)\n",
    "                        y_all = torch.cat([y_mix, labels_a], 0)\n",
    "                        inputs, labels = shuffle(x_all, y_all)\n",
    "                    elif(methodList[methodIndex] == 'automix' and phase == 'train'):\n",
    "                        midIdx = int(len(inputs_a) / 2)\n",
    "                        inputs_unseen = torch.cat([inputs_a[midIdx:]], 0)\n",
    "                        labels_unseen = torch.cat([labels_a[midIdx:]], 0)\n",
    "                        inputs_a, inputs_b = inputs_a[:midIdx], inputs_b[:midIdx]\n",
    "                        labels_a, labels_b = labels_a[:midIdx], labels_b[:midIdx]\n",
    "                        \n",
    "                        lam = torch.rand(len(labels_a), 1).to(device)\n",
    "                        y_mix = lam * labels_a + (1 - lam) * labels_b\n",
    "                        x_mix = unet([inputs_a, inputs_b, y_mix])\n",
    "                        \n",
    "                        x_all = torch.cat([x_mix, inputs_a, inputs_b], 0)\n",
    "                        y_all = torch.cat([y_mix, labels_a, labels_b], 0)\n",
    "                        inputs, labels = shuffle(x_all, y_all)\n",
    "\n",
    "                        if(i in [0, 1]):\n",
    "                            cmap = 'gray'\n",
    "                            print(lam[0].item())\n",
    "                            plt.subplot(131)\n",
    "                            plt.imshow(inputs_a[0].permute(1, 2, 0).squeeze().cpu().detach().numpy(), cmap=cmap)\n",
    "                            plt.subplot(132)\n",
    "                            plt.imshow(inputs_b[0].permute(1, 2, 0).squeeze().cpu().detach().numpy(), cmap=cmap)\n",
    "                            plt.subplot(133)\n",
    "                            plt.title(y_mix[0].cpu())\n",
    "                            plt.imshow(x_mix[0].permute(1, 2, 0).squeeze().cpu().detach().numpy(), cmap=cmap)\n",
    "                            plt.show()\n",
    "                    outputs = net(inputs)\n",
    "                    preds = torch.argmax(outputs, 1)\n",
    "                    if(methodList[methodIndex] in ['mixup'] and phase == 'train'):\n",
    "                        clsLoss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "                    else:\n",
    "                        clsLoss = criterion(outputs, labels)\n",
    "                    if(methodList[methodIndex] == 'automix' and phase == 'train'):\n",
    "#                         x_lam = lam.view(len(lam), 1, 1, 1)\n",
    "#                         mix = inputs_a * x_lam + inputs_b * (1 - x_lam)\n",
    "                        d1 = norm1(inputs_a, x_mix)\n",
    "                        d2 = norm1(inputs_b, x_mix)\n",
    "#                         disLoss = lam[:int(lam.shape[0]/3)] * d1 + (1 - lam[:int(lam.shape[0]/3)]) * d2\n",
    "                        disLoss = lam * d1 + (1 - lam) * d2\n",
    "#                         disLoss = d1 + d2\n",
    "                        loss = clsLoss.mean() + disLoss.mean()\n",
    "#                         loss = clsLoss.mean() + norm1(mix, x_mix).mean()\n",
    "                    elif(methodList[methodIndex] == 'adamixup' and phase == 'train'):\n",
    "                        x_pos = torch.cat([inputs_unseen, inputs_a], 0)\n",
    "                        y_pos = torch.zeros(len(x_pos), 2).scatter_(1, torch.ones(len(x_pos), 1).long(), 1).to(device)\n",
    "                        x_neg = torch.cat([x_mix], 0)\n",
    "                        y_neg = torch.zeros(len(x_neg), 2).scatter_(1, torch.zeros(len(x_neg), 1).long(), 1).to(device)\n",
    "                        x_bin = torch.cat([x_pos, x_neg], 0)\n",
    "                        y_bin = torch.cat([y_pos, y_neg], 0)\n",
    "                        x_bin, y_bin = shuffle(x_bin, y_bin)\n",
    "                        linear = nn.Linear(num_classes, 2)\n",
    "                        extra = net(x_bin)\n",
    "                        logits = net.linear2(extra)\n",
    "                        disLoss = criterion(logits, y_bin)\n",
    "                        \n",
    "                        loss = clsLoss.mean() + disLoss.mean()\n",
    "                    else:\n",
    "                        loss = clsLoss.mean()\n",
    "                    if(phase == 'train'):\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss.append(loss.cpu().item())\n",
    "                if(methodList[methodIndex] in ['mixup'] and phase == 'train'):\n",
    "                    num_correct = (lam.cpu().item() * preds.eq(torch.argmax(labels_a, 1)).cpu().sum().float().item() + (1 - lam.cpu().item()) * preds.eq(torch.argmax(labels_b, 1)).cpu().sum().float().item())\n",
    "                else:\n",
    "                    num_correct = preds.eq(torch.argmax(labels, 1)).cpu().sum().float().item()\n",
    "                running_corrects += num_correct\n",
    "                running_cnt += inputs.shape[0]\n",
    "\n",
    "                sys.stdout.write('                                                                                                 \\r')\n",
    "                sys.stdout.flush()\n",
    "                sys.stdout.write('Iter: {} / {} ({:.0f}s)\\tLoss= {:.4f}\\tAcc= {:.2f}%\\r'\n",
    "                     .format(i+1, iterNum[phase], time.time() - epochStart, \n",
    "                             running_loss[-1], 100 * num_correct / len(inputs)))\n",
    "                sys.stdout.flush()\n",
    "            epoch_loss = np.mean(running_loss)\n",
    "            epoch_acc = running_corrects / running_cnt\n",
    "            accLog[phase].append(epoch_acc)\n",
    "            lossLog[phase].append(epoch_loss)\n",
    "            epochDuration = time.time() - epochStart\n",
    "            epochStart = time.time()\n",
    "            sys.stdout.write('                                                                                                  \\r')\n",
    "            sys.stdout.flush()\n",
    "            print('[ {} ] Loss: {:.4f} Acc: {:.2f}% ({:.0f}mins {:.2f}s)'\n",
    "                  .format(phase, epoch_loss, 100 * epoch_acc, epochDuration // 60, epochDuration % 60))\n",
    "\n",
    "            if(phase == 'val' and epoch_acc > best_acc):\n",
    "                print('Saving best model to {}'.format(os.path.join(modelPath, modelName)))\n",
    "                if(methodList[methodIndex] == 'automix'):\n",
    "                    state = {'net': [net, unet], 'opt': optimizer, 'acc': epoch_acc, 'epoch': epoch}\n",
    "                elif(methodList[methodIndex] == 'adamixup'):\n",
    "                    state = {'net': [net, PRG_net], 'opt': optimizer, 'acc': epoch_acc, 'epoch': epoch}\n",
    "                else:\n",
    "                    state = {'net': net, 'opt': optimizer, 'acc': epoch_acc, 'epoch': epoch}\n",
    "                if not os.path.isdir(modelPath):\n",
    "                    os.makedirs(modelPath)\n",
    "                torch.save(state, os.path.join(modelPath, modelName))\n",
    "                best_acc = epoch_acc\n",
    "            if(phase == 'val' and epoch == n_epochs - 1):\n",
    "                finalModelName = 'final-{}'.format(modelName)\n",
    "                print('Saving final model to {}'.format(os.path.join(modelPath, finalModelName)))\n",
    "                if(methodList[methodIndex] == 'automix'):\n",
    "                    state = {'net': [net, unet], 'opt': optimizer, 'acc': epoch_acc, 'epoch': epoch}\n",
    "                elif(methodList[methodIndex] == 'adamixup'):\n",
    "                    state = {'net': [net, PRG_net], 'opt': optimizer, 'acc': epoch_acc, 'epoch': epoch}\n",
    "                else:\n",
    "                    state = {'net': net, 'opt': optimizer, 'acc': epoch_acc, 'epoch': epoch}\n",
    "                if not os.path.isdir(modelPath):\n",
    "                    os.makedirs(modelPath)\n",
    "                torch.save(state, os.path.join(modelPath, finalModelName))\n",
    "        print()\n",
    "    duration = time.time() - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.2f}s'.format(duration // 3600, (duration % 3600) // 60, duration % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    log = dict({'acc': accLog, 'loss': lossLog})\n",
    "    with open(os.path.join(modelPath, '{}-log-{}.pkl'.format(methodList[methodIndex], fold+1)), 'wb') as f:\n",
    "        pickle.dump(log, f)\n",
    "\n",
    "    return best_acc, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "modelPath = 'pytorch_model_learnt/{}/{}/{}'.format(dataName[dataIndex], netList[netIndex], methodList[methodIndex])\n",
    "for fold in tqdm_notebook(range(1), desc='Fold'):\n",
    "    modelName = '{}-{}.ckpt'.format(methodList[methodIndex], fold+1)\n",
    "    if(netList[netIndex] == 'MyNet'):\n",
    "        net = MyNet(input_shape=shape, \n",
    "                    num_classes=num_classes)\n",
    "    elif(netList[netIndex] == 'ResNet18'):\n",
    "        net = ResNet18(num_classes=num_classes)\n",
    "    if(methodList[methodIndex] == 'adamixup'):\n",
    "        net.linear2 = nn.Linear(num_classes, 2)    \n",
    "    net = net.to(device)\n",
    "    parameters = [{'params': net.parameters()}]\n",
    "#     summary(model=net, input_size=shape)\n",
    "\n",
    "    if(device.type == 'cuda'):\n",
    "    #     net = torch.nn.DataParallel(net)\n",
    "        cudnn.benchmark = True\n",
    "    if(methodList[methodIndex] == 'automix'):\n",
    "        unet = UNet(input_shape=(shape[0], shape[1], shape[2]), output_shape=shape, num_classes=num_classes)\n",
    "        unet = unet.to(device)\n",
    "        parameters.append({'params': unet.parameters()})\n",
    "    if(methodList[methodIndex] == 'adamixup'):\n",
    "        PRG_net = ResNet18(num_classes=3)\n",
    "        PRG_net = PRG_net.to(device)\n",
    "        parameters.append({'params': PRG_net.parameters()})\n",
    "#         summary(model=PRG_net, input_size=shape)\n",
    "\n",
    "    criterion = myLoss(methodList[methodIndex])\n",
    "    optimizer = optim.SGD(parameters, lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "#     optimizer = optim.Adam(parameters, lr=0.0002, betas=(0.5, 0.999))\n",
    "    exp_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, lrDecayStep, gamma=0.1)\n",
    "    \n",
    "    best_acc, log = train_val(optimizer, \n",
    "                              n_epochs, \n",
    "                              trainDataset, \n",
    "                              trainLoader, \n",
    "                              testDataset, \n",
    "                              testLoader)\n",
    "    plot_acc_loss(log, 'both', '{}-'.format(methodList[methodIndex]), '-{}'.format(fold+1))\n",
    "    plot_acc_loss(log, 'loss', '{}-'.format(methodList[methodIndex]), '-{}'.format(fold+1))\n",
    "    plot_acc_loss(log, 'accuracy', '{}-'.format(methodList[methodIndex]), '-{}'.format(fold+1))\n",
    "    if(methodList[methodIndex] == 'automix'):\n",
    "        net, unet = torch.load(os.path.join(modelPath, modelName))['net']\n",
    "    elif(methodList[methodIndex] == 'adamixup'):\n",
    "        net, PRG_net = torch.load(os.path.join(modelPath, modelName))['net']\n",
    "    else:\n",
    "        net = torch.load(os.path.join(modelPath, modelName))['net']\n",
    "    accTotal = eval_total(net, testLoader)\n",
    "    accPerClass = eval_per_class(net, testLoader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = myLoss(methodList[methodIndex])\n",
    "optimizer = optim.SGD(parameters, lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "exp_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, lrDecayStep, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape)\n",
    "print('==> Building model..')\n",
    "modelPath = 'pytorch_model_learnt/{}/{}/{}'.format(dataName[dataIndex], netList[netIndex], methodList[methodIndex])\n",
    "if(netList[netIndex] == 'MyNet'):\n",
    "    net = MyNet(input_shape=shape, \n",
    "                num_classes=num_classes)\n",
    "elif(netList[netIndex] == 'ResNet18'):\n",
    "    net = ResNet18(num_classes=num_classes)\n",
    "if(methodList[methodIndex] == 'adamixup'):\n",
    "    net.linear2 = nn.Linear(num_classes, 2)\n",
    "net = net.to(device)\n",
    "parameters = [{'params': net.parameters()}]\n",
    "        \n",
    "if(device.type == 'cuda'):\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "summary(model=net, input_size=shape)\n",
    "\n",
    "if(methodList[methodIndex] == 'automix'):\n",
    "    model_dict = net.state_dict()\n",
    "    checkpoint = 'pytorch_model_learnt/{}/{}/baseline/baseline-1.ckpt'.format(dataName[dataIndex], netList[netIndex])\n",
    "    pretrained_dict = torch.load(checkpoint)['net'].state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    net.load_state_dict(model_dict)\n",
    "#     for k,v in net.named_parameters():\n",
    "#         if(k != 'linear.weight' and k != 'linear.bias'):\n",
    "#             v.requires_grad = False\n",
    "#     parameters = [{'params': filter(lambda p: p.requires_grad, net.parameters())}]\n",
    "#     fnet = FeatureNet(input_shape=shape)\n",
    "#     fnet = fnet.to(device)\n",
    "#     parameters.append({'params': fnet.parameters()})\n",
    "    unet = UNet(input_shape=(shape[0]*2, shape[1], shape[2]), output_shape=shape, num_classes=num_classes)\n",
    "    unet = unet.to(device)\n",
    "    parameters.append({'params': unet.parameters()})\n",
    "#     summary(model=unet, input_size=((shape[0]*2, shape[1], shape[2]), num_classes))\n",
    "if(methodList[methodIndex] == 'adamixup'):\n",
    "    PRG_net = ResNet18(num_classes=3)\n",
    "    PRG_net = PRG_net.to(device)\n",
    "    parameters.append({'params': PRG_net.parameters()})\n",
    "    summary(model=PRG_net, input_size=shape)\n",
    "    \n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
